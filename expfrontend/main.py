import requests
import datetime 
import json
import pandas as pd
import streamlit as st
from streamlit.components.v1 import html

from confirm_button_hack import cache_on_button_press

import re

#í˜ì´ì§€ íƒ€ì´í‹€
st.set_page_config(page_title="News Summarization",layout = 'wide')
#ìŠ¤í¬ë¡¤
st.markdown("""
                <html>
                    <head>
                    <style>
                        ::-webkit-scrollbar {
                            width: 10px;
                            }

                            /* Track */
                            ::-webkit-scrollbar-track {
                            background: #f1f1f1;
                            }

                            /* Handle */
                            ::-webkit-scrollbar-thumb {
                            background: #888;
                            }

                            /* Handle on hover */
                            ::-webkit-scrollbar-thumb:hover {
                            background: #555;
                            }
                    </style>
                    </head>
                    <body>
                    </body>
                </html>
            """, unsafe_allow_html=True)

with open("style.css") as source_css:
        st.markdown(f"<style>{source_css.read()}</style>",unsafe_allow_html=True)

#ê²€ìƒ‰í˜ì´ì§€
def search_page():    
    #Googleì²˜ëŸ¼ ì–´í”Œ ì œëª©ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ë“¯
    st.markdown("<h1 style='text-align: center;'>NEWSUMMARY</h1>", unsafe_allow_html=True)
    search_contain = st.empty()
    news_contain = st.empty()
    if 'company_name' not in st.session_state:
        st.session_state.company_name = ""
    if 'before_company_name' not in st.session_state:
        st.session_state.before_company_name = ""
    if 'before_search_date' not in st.session_state:
        st.session_state.before_search_date = (datetime.date(2022,12,1), datetime.date(2022,12,15))
    
    page_buttons=[]
    with search_contain.container():
        #ê²€ìƒ‰ì°½
        company_name = st.text_input("ê²€ìƒ‰", value=st.session_state['company_name'], placeholder ="íšŒì‚¬ëª… ì…ë ¥",label_visibility='collapsed', key="company_name")
        #ê¸°ê°„ ê²€ìƒ‰ì°½
        _,col1,col2 = st.columns([15,1,2])
        search_date = col2.date_input("ê¸°ê°„",value=st.session_state.before_search_date,label_visibility='collapsed', key = "search_date")
        
        #news_num = col1.number_input("ë‰´ìŠ¤ ê°œìˆ˜",0, 999,999,label_visibility='collapsed',key = "news_num")
        
        if st.session_state.company_name != "" and len(search_date) > 1 :   #ê²€ìƒ‰í–ˆìœ¼ë©´
            #ê²€ìƒ‰ì–´ë‚˜ ê²€ìƒ‰ê¸°ê°„ì´ ë°”ë€Œë©´ newë°ì´í„° ìƒˆë¡œ ë°›ê¸°
            if st.session_state.before_company_name != st.session_state.company_name or st.session_state.before_search_date !=st.session_state.search_date:
                st.session_state.before_company_name = st.session_state.company_name
                st.session_state.before_search_date = st.session_state.search_date
                
                start_date = f"{st.session_state.search_date[0].year:0>4d}{st.session_state.search_date[0].month:0>2d}{st.session_state.search_date[0].day:0>2d}"  #ì‹œì‘ê²€ìƒ‰ì¼
                end_date = f"{st.session_state.search_date[1].year:0>4d}{st.session_state.search_date[1].month:0>2d}{st.session_state.search_date[1].day:0>2d}"    #ì¢…ë£Œê²€ìƒ‰ì¼
                # íšŒì‚¬ì´ë¦„ ê²€ìƒ‰ ìš”ì²­
                #response = requests.post(f"http://localhost:8001/company_name/?company_name={st.session_state.company_name}&date_gte={start_date}&date_lte={end_date}&news_num=9999")
                #response = response.json()
                #news_df = pd.read_json(response["news_df"],orient="records")
                #topic_df = pd.read_json(response["topic_df"],orient="records")
                news_df = pd.read_pickle("news_df.pkl")
                topic_df = pd.read_pickle("topic_df.pkl")
                st.session_state["news_df"] = news_df
                st.session_state["topic_df"] = topic_df
            #ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ê²°ê³¼ê°€ ì—†ë‹¤ê³  ë°˜í™˜
            if len(st.session_state["news_df"]) == 0:
                st.warning('ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.', icon="âš ï¸")
            
            #dataframe ë³´ê¸°            
            #st.write(st.session_state["news_df"])
            #st.write(st.session_state["topic_df"])
            
            #ë²„íŠ¼ ì¶”ê°€  
            label_to_icon = {"negative":"ğŸ˜•","neutral":"ğŸ˜","positive":"ğŸ˜ƒ"}
            col1, col2 = st.columns([1,1])
            max_idx = len(st.session_state["topic_df"]) 
            for idx in range(max_idx):
                topic_sentiment = st.session_state["topic_df"]["sentiment"][idx]
                topic_number = st.session_state["topic_df"]["topic"][idx]
                topic_text = st.session_state["topic_df"]["one_sent"][idx]
                page_buttons.append(idx)
                if idx%2 == 0:
                    col1.button(label_to_icon[topic_sentiment] + topic_text,key=idx)
                else:
                    col2.button(label_to_icon[topic_sentiment] + topic_text,key=idx)

    # ìš”ì•½ë¬¸ ëˆ„ë¥´ë©´ í•´ë‹¹ í˜ì´ì§€ë¡œ
    for button_key in page_buttons:
        if st.session_state[button_key]:
            search_contain.empty()
            with news_contain.container():
                news_page(button_key)
    

#ë‰´ìŠ¤ ìš”ì•½ í˜ì´ì§€
def news_page(idx):
    #í•œì¤„ìš”ì•½(ì œëª©)
    topics_text = st.session_state["topic_df"]["one_sent"][idx]
    topic_number = int(st.session_state["topic_df"]["topic"][idx])
    st.subheader(topics_text)
    _, col2 = st.columns([7,1])
    back_button = col2.button("back")
    if back_button:
        page_buttons.clear()
        news_contain.empty()

    #ë‰´ìŠ¤ë§í¬ [date,title,url]
    news_df = st.session_state["news_df"]
    news_list = news_df[news_df['topic'] == topic_number]
    news_list = news_list.reset_index(drop=True)
    with st.expander("ë‰´ìŠ¤ ë§í¬"):
        for _, row in news_list[:12].iterrows():
            col1, col2 = st.columns([1,5])
            col1.text(row['date'])
            col2.caption(f"<a href='{row['url']}'>{row['title']}</a>",unsafe_allow_html=True)    
   
    #ìš”ì•½ë¬¸
    st.subheader("ìš”ì•½ë¬¸")
    now_news_df = news_list[['context']]
    now_news_json = now_news_df.to_json(orient = "columns",force_ascii=False)
    #summarization = requests.post(f"http://localhost:8001/summary/",json=now_news_json)
    #summary_text = summarization.json()["summarization"]
    summary_text = """
    ì‚¼ì„±ì „ìê°€ 3ì¼ ì£¼ì£¼ì´íšŒë¥¼ ê°œìµœí•˜ê³  ìœ ëª…í¬ ì „ ì‚°ì—…ë¶€ í†µìƒêµì„­ë³¸ê³¼ í—ˆì€ë…• ì„œìš¸ëŒ€ ê³µëŒ€ ì‚¬ì™¸ì´ì‚¬ ì„ ì„ì„ ì˜ê²°í–ˆë‹¤.

    ì‚¼ì„±ì „ìëŠ” 3ì¼ ì£¼ì£¼ì´íšŒë¥¼ ì—´ê³  í—ˆì€ë…• ì„œìš¸ëŒ€ ê³µëŒ€ ì™€ ìœ ëª…í¬ ì „ ì‚°ì—…ë¶€ í†µìƒêµì„­ë³¸ê³¼ ì‚°ì—…í†µìƒìì›ë¶€ í†µìƒêµì„­ë³¸ì„ ì‚¬ì™¸ì´ì‚¬ë¡œ ì„ ì„í–ˆë‹¤.

    2017ë…„ë¶€í„° 2019ë…„ê¹Œì§€ í•™íšŒ ë¶€íšŒì¥ì„ ì§€ëƒˆìœ¼ë©°, í•œêµ­í˜ì‹ í•™íšŒ íšŒì¥ê³¼ í•™íšŒ íšŒì¥ ë“±ì„ ì§€ë‚¸ ì—ë„ˆì§€ ë¶€ë¬¸ì˜ ì„í•™ìœ¼ë¡œ ì†ê¼½íŒë‹¤.

    ì‚¼ì„±ì „ìëŠ” 3ì¼ ìš©ì¸ ì‚¼ì„±ì¸ì¬ê°œë°œì›ì—ì„œ ì„ì‹œ ì£¼ì£¼ì´íšŒë¥¼ ì—´ê³  ìœ ëª…í¬ ì „ ì‚°ì—…í†µìƒìì›ë¶€ í†µìƒêµì„­ë³¸ê³¼ í—ˆì€ë…• ì„œìš¸ëŒ€ ê³µëŒ€ ë¥¼ ì‚¬ì™¸ì´ì‚¬ë¡œ ì„ ì„í–ˆë‹¤. ì´ì¬ìš© íšŒì¥ ì·¨ì„ ì´í›„ ì—´ë¦° ì´ë²ˆ ì„ì‹œì£¼ì´ì— ëŒ€í•´ ë‚´ë…„ 3ì›” ì •ê¸°ì£¼ì´ì—ì„œ ì´ íšŒì¥ì„ ë“±ê¸°ì´ì‚¬ë¡œ ì„ ì„í•˜ê¸° ìœ„í•œ ì‚¬ì „ì‘ì—…ì´ ì•„ë‹ˆëƒëŠ” ë¶„ì„ì´ ë‚˜ì˜¤ê³  ìˆë‹¤.

    ì‚¼ì„±ì „ìëŠ” 3ì¼ ì‚¬ì™¸ì´ì‚¬ ì„ ì„ìœ¼ë¡œ ê²¬ì œ ë° ê°ì‹œê¸°ëŠ¥ì´ ê°•í™”ë˜ë©´ì„œ ì‚¼ì„±ì „ìì˜ ê²½ì˜ íˆ¬ëª…ì„± í™•ë³´ì™€ ì†Œì•¡ì£¼ì£¼ ë³´í˜¸ ì—­í• ì´ í™•ëŒ€ë  ê²ƒìœ¼ë¡œ í‰ê°€í–ˆë‹¤.

    ì§€ë‚œ 3ì›” ì£¼ì£¼ì´íšŒ ì´í›„ ì‚¬ì™¸ì´ì‚¬ 4ëª…, ì‚¬ë‚´ì´ì‚¬ 5ëª…ìœ¼ë¡œ ì´ì‚¬íšŒë¥¼ ìš´ì˜í•´ ì˜¤ë˜ ì‚¼ì„±ì „ìê°€ 5ì›” ë°•ë³‘êµ­ ì‚¬ì™¸ì´ì‚¬ê°€ 5ì›” ë³„ì„¸í•˜ê³  í•œí™”ì§„ ì‚¬ì™¸ì´ì‚¬ê°€ ìƒˆ ì •ë¶€ì˜ ì´ˆëŒ€ í™˜ê²½ë¶€ ì§ì„ ë§¡ìœ¼ë©´ì„œ ì‚¬ì„í•´ 6ëª…ì˜ ì‚¬ì™¸ì´ì‚¬ ì¤‘ ê²°ì› 2ëª…ì´ ìƒê²¼ë‹¤ê³  ë°íˆë©° ì„ì‹œ ì£¼ì£¼ì´íšŒë¥¼ ê°œìµœí–ˆë‹¤. í•™íšŒ ë¶€íšŒì¥, í•œêµ­í˜ì‹ í•™íšŒ íšŒì¥, í•™íšŒ íšŒì¥ì„ ì—­ì„í•œ ì—ë„ˆì§€ ì „ë¬¸ê°€ì¸ ë§›í—ˆ ì‚¬ì™¸ì´ì‚¬ëŠ” ì „ë¬¸ê°€ë‹¤.
    """
    st.write(summary_text)
    #í‚¤ì›Œë“œ
    st.subheader("í‚¤ì›Œë“œ")
    

if __name__ == '__main__':
    search_page()
        
        